# -*- coding: utf-8 -*-
"""EXCERCISE_2_NOTEBOOK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VE0gufJEA51jkbL5bUURBnEboewtkYFc
"""

import pandas as pd
import numpy as np
import io
import seaborn as sns 
import matplotlib.pyplot as plt 

from google.colab import files
uploaded = files.upload()

df = pd.read_csv(io.BytesIO(uploaded['lymphography_data.csv']))
print(df.head())
print(df.shape)

"""# **Data preparation & Visualisation**

Checking for null values, outliers, datatypes etc.
"""

#lets see the data
print(df.head(5))

#rows and columns
df.shape

# creating a corr plot to help understand relationships
plt.figure(figsize=(14,12))
corr_plot = sns.heatmap(df.corr(),linewidths=0.1,vmax=1.0, 
            square=True,  linecolor='white', annot=True)
plt.show()

# apply the dtype attribute
result = df.dtypes

print("Output:")
print(result)

#Class value counts
df['Class'].value_counts()

# We can now say the following;

# Class 1 = Normal
# Class 2 = Metastases
# Class 3 = Malign Lymph
# Class 4 = Fibrosis


# And Lets visualize the Class counts
sns.countplot(df['Class'])
plt.title("Class Counts")
plt.show()

# Lets drop the classes we don't need to reduce noise
index_names = df[ df['Class'] == 1 ].index
  
# drop these row indexes
# from dataFrame
df.drop(index_names, inplace = True)

# Lets drop the other class we don't need to reduce noise
index_names = df[ df['Class'] == 4 ].index
  
# drop these row indexes
# from dataFrame
df.drop(index_names, inplace = True)

# And now Lets visualize the Class counts
sns.countplot(df['Class'])
plt.title("Class Counts")
plt.show()

df.shape

display(df)

print(df.columns)

np.where(pd.isnull(df))

#Creating 0/1 class identification

# Class 0 = Metastases
# Class 1 = Malign Lymph

df['Class Ind'] = 0
df.loc[df['Class']==2,'Class Ind'] = 0
df.loc[df['Class']==3,'Class Ind'] = 1

# Lets do a quick check - looking good

df['Class Ind'].value_counts()

df = df.drop(['Class'] ,axis=1)

print(df.columns)

"""# **Feature Selection**
Lets look at feature selection, I tried PCA, LDA, Feature Importance, Chi2 worked best. I took two different appraches to visualise Chi2 with the same result, one more manual and one more standard.
"""

from sklearn.feature_selection import SelectKBest,chi2

dff = df[['Lymphatics', 'block of affere', 'bl. of lymph. c', 'bl. of lymph. s',
       'by pass', 'extravasates', 'regeneration of', 'early uptake in',
       'lym.nodes dimin', 'lym.nodes enlar', 'changes in lym',
       ' defect in node', ' changes in node', 'changes in stru',
       'special forms', 'dislocation of', 'exclusion of no','no. of nodes in','Class Ind']]

best = SelectKBest(chi2,k=18)
best.fit(dff[['Lymphatics', 'block of affere', 'bl. of lymph. c', 'bl. of lymph. s',
       'by pass', 'extravasates', 'regeneration of', 'early uptake in',
       'lym.nodes dimin', 'lym.nodes enlar', 'changes in lym',
       ' defect in node', ' changes in node', 'changes in stru',
       'special forms', 'dislocation of', 'exclusion of no', 'no. of nodes in']], dff['Class Ind'])

df_score = pd.DataFrame(best.pvalues_,columns=['p_values'])
df_score['chi2_values'] = best.scores_
df_score['columns'] = ['Lymphatics', 'block of affere', 'bl. of lymph. c', 'bl. of lymph. s',
       'by pass', 'extravasates', 'regeneration of', 'early uptake in',
       'lym.nodes dimin', 'lym.nodes enlar', 'changes in lym',
       ' defect in node', ' changes in node', 'changes in stru',
       'special forms', 'dislocation of', 'exclusion of no', 'no. of nodes in']
df_score.sort_values(by='p_values')

#We have value of significance, alpha =0.05. so we have to choose those features with p value <= alpha

df_score[df_score['p_values'] <= 0.05]['columns']

#Lets look at another methodology to see if the result is the same

X = df.drop('Class Ind',axis=1)
y = df['Class Ind']

chi_scores = chi2(X,y)

#turns scores into an array
chi_scores

p_values = pd.Series(chi_scores[1],index = X.columns)
p_values.sort_values(ascending = False , inplace = True)

#bar chart of features, loest are most relevant as distance is shorter
p_values.plot.bar()

#Both have the same features indicated - lets look at the top 8 (itested top 5 but the accuracy was a little less)

print(df.columns)

#Key Features: 'no. of nodes in', 'changes in stru', 'lym.nodes enlar', 
#'block of affere','special forms', 'early uptake in','dislocation of'

#dropping unwanted columns from independent variables, redcues noise
x_vars = df.drop(['Class Ind', 'lym.nodes dimin','bl. of lymph. s','extravasates',
                 ' defect in node',' changes in node','Lymphatics','regeneration of','changes in lym',
                 'by pass','bl. of lymph. c'] ,axis=1)
print(x_vars)

#independent variable
y_var = df['Class Ind']
print(y_var)

print(x_vars.columns)

"""# **Model Preperation**"""

pip install pgmpy

#importaing libraries
from sklearn.model_selection import train_test_split
from pgmpy.estimators import BDeuScore, K2Score, BicScore
from pgmpy.models import BayesianModel

# 70% training as requested
x_train,x_test,y_train,y_test = train_test_split(x_vars,y_var, train_size = 0.7,random_state=42)

print(x_train)

print(y_train)

print(type(x_train))
print(type(y_train))
data = x_train.join(y_train)
print(data)

"""*Loading estimators to build our model, I tried both maximum liklihood and Bayesian - which is essentially a fight between frequentist and bayseian prbabability.The Hill Climbing Algorithm is a local search algorithm that continuously moves towards increasing heights/values ​​to find the top of a mountain or the best solution to a problem. Terminates when a peak is reached where there are no neighbors with higher values*


"""

pip install networkx

pip install pylab

from pgmpy.models import BayesianNetwork
import pandas as pd
from IPython.display import Image, display
from pgmpy.estimators import HillClimbSearch, BicScore, PC, K2Score
from pgmpy.estimators import BayesianEstimator
from pgmpy.estimators import ParameterEstimator
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination

import pylab as plt
import networkx as nx

#Calculating scores
bdeu = BDeuScore(data, equivalent_sample_size=10)
k2 = K2Score(data)
bic = BicScore(data)

# HC Implementations
hc = HillClimbSearch(data)
best_model = hc.estimate(scoring_method='K2Score')
print(sorted(best_model.nodes()))
print(best_model.edges())
display(Image((nx.drawing.nx_pydot.to_pydot(best_model)).create_png()))

print(type(best_model))
model_final = BayesianNetwork(best_model)
print(type(model_final))

# Scoring the parameters - nodes
pe = ParameterEstimator(best_model, data)
model_final.fit(data, estimator=BayesianEstimator, prior_type="BDeu") #
for cpd in model_final.get_cpds():
    print(cpd)

x_test.shape

#Printing the model using PyLab
#nx.draw(model_final, with_labels=True)

# Create a bayesian model with nodes and edges.
#model_final = BayesianNetwork([('early uptake in', 'block of affere'), ('lym.nodes enlar', 'early uptake in'), ('lym.nodes enlar', 'changes in stru'), ('lym.nodes enlar', 'special forms'), ('dislocation of', 'early uptake in'), ('exclusion of no', 'dislocation of'), ('exclusion of no', 'no. of nodes in'), ('no. of nodes in', 'Class Ind'), ('no. of nodes in', 'lym.nodes enlar'), ('Class Ind', 'special forms'), ('Class Ind', 'block of affere')])

# Estimate the CPD for each variable based on a given data set.
#model_final.fit(data, estimator=BayesianEstimator)
#fig, ax = plt.subplots(figsize=(15, 10))


#pos = nx.spring_layout(model_final) 
#nx.draw(model_final, pos=pos, with_labels=True, node_size = 13000, font_size = 15, arrowsize=20, node_color='red', ax=ax)

# Check the model for various errors. This method checks for the following errors.
# * Checks if the sum of the probabilities for each state is equal to 1 (tol=0.01).
# * Checks if the CPDs associated with nodes are consistent with their parents.
model_final.check_model()

x_test

from pandas.core.groupby.groupby import DataFrame
infer = VariableElimination(model_final)

print(type(x_test))
print(type(y_test))
print(x_test.iat[0,0])

num_rows, num_cols = x_test.shape
print(num_rows)
posterior_py = np.empty(shape=(num_rows,2))
print(posterior_py.shape)

x_test

print(x_test.columns)

result = infer.query(['Class Ind'], evidence={'block of affere': x_test.iat[0,0],
                                              'early uptake in': x_test.iat[0,1],
                                              'lym.nodes enlar': x_test.iat[0,2],
                                              'changes in stru': x_test.iat[0,3],
                                              'special forms'  : x_test.iat[0,4],
                                              'dislocation of' : x_test.iat[0,5],
                                              'exclusion of no': x_test.iat[0,6],
                                              'no. of nodes in': x_test.iat[0,7] })
print(result)
#,'changes in stru': x_test.iat[0,3]

print(result.values[0])
print(result.values[1])

print(type(y_test))

# I tried to propagate the parent only (no of nodes in), since the other are children othe class Ind, but the accuracy was way less
for i in range(num_rows):
  try:
    print('block of affere', x_test.iat[i,0],'early uptake in', x_test.iat[i,1],'lym.nodes enlar', x_test.iat[i,2],
          'changes in stru', x_test.iat[i,3],'special forms'  , x_test.iat[i,4],'dislocation of' , x_test.iat[i,5],
          'exclusion of no', x_test.iat[i,6],'no. of nodes in', x_test.iat[i,7])

    result = infer.query(['Class Ind'], evidence={'block of affere': x_test.iat[i,0],'early uptake in': x_test.iat[i,1],
                                                  'lym.nodes enlar': x_test.iat[i,2],'changes in stru': x_test.iat[i,3],
                                                  'special forms'  : x_test.iat[i,4],'dislocation of' : x_test.iat[i,5],
                                                  'exclusion of no': x_test.iat[i,6],'no. of nodes in': x_test.iat[i,7]})
    
    posterior_py[i]= result.values[0],result.values[1]
  except KeyError as exception:
      print('KeyError: {} does not exist in the column {}.'.format(str(exception),i))
      continue

#print(posterior_py)

#Getting the final vector to compare
print(type(y_train))

y_pred=list()
for i in range(num_rows):
  if posterior_py[i][0]>posterior_py[i][1]:
    y_pred.append(0) #The category zero was the one with the highest probability
  else:
    y_pred.append(1) #The category one was the one with the highest probability

ypred = pd.core.series.Series(y_pred)

#Prediction table
print(ypred)

from sklearn.metrics import classification_report
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, ypred)
sns.heatmap(cm,annot=True,cmap="Blues",fmt="d",cbar=False)
#accuracy score
from sklearn.metrics import accuracy_score
ac=accuracy_score(y_test, y_pred)
print('accuracy of the model: ',ac)

print(classification_report(y_test,ypred))

# Class 0 = Metastases
# Class 1 = Malign Lymph

# Causal inferencing

from pgmpy.inference import BeliefPropagation

class_infer = BeliefPropagation(model_final)

# What"s the probability of a candidate getting a strong letter of recommendation
q = class_infer.query(variables=['Class Ind'], evidence=None)
print(q)

# What"s the probability of a patient having Metastese or Malign Lymph based on Lymph Nodes Enlarged
q = class_infer.query(variables=['Class Ind'], evidence={'lym.nodes enlar':4})
print(q)

#saving pickled model
import pickle
filename = 'my_finalized_model.sav'
pickle.dump(model_final, open(filename, 'wb'))

"""# **Artificial Nueral Network using Keras**"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

#pip install keras

x_test.shape

#pip install tensorflow

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
import keras
from keras.models import Sequential
from keras.layers import Dense
import warnings

classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(11, activation='relu', kernel_initializer='glorot_uniform',input_dim=8))

# Adding the second hidden layer
classifier.add(Dense(11, activation='relu', kernel_initializer='glorot_uniform',input_dim=8))

# Adding the output layer
classifier.add(Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform',input_dim=8))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

classifier.fit(x_train, y_train, batch_size = 32, epochs = 50)

# Predicting the Test set results
y_pred = classifier.predict(x_test)

cm = confusion_matrix(y_test, y_pred.round())
sns.heatmap(cm,annot=True,cmap="Blues",fmt="d",cbar=False)
#accuracy score
from sklearn.metrics import accuracy_score
ac=accuracy_score(y_test, y_pred.round())
print('accuracy of the model: ',ac)

# Nueral network
print(classification_report(y_test,y_pred.round()))

# Bayseian Network
print(classification_report(y_test,ypred))